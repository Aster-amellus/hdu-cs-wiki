# 基本概念介绍
前面已经介绍过，强化学习难入坑的的原因之一就在于概念繁多。下面将进行基本概念的介绍，本章节最好能够理解，不理解也没有关系，但是建议作为参考章节常看常新。后续章节不理解某个概念时，便回来看看，相信一定能够做到常看常新、从而加深你对于概念的理解。下面将进行四个部分的介绍，分别为强化学习的基本过程、强化学习的基本组成内容、强化学习的基本概念以及强化学习的目标。


## 强化学习的基本过程
前面已经介绍过强化学习的核心过程，在于智能体与环境进行交互，通过给出的奖励反馈作为信号学习的过程。简单地用图片表示如下：
![](static/4.6.9.3.1.png)
正是在这个与环境的交互过程中，智能体不断得到反馈，目标就是尽可能地让环境反馈的奖励足够大。

## 强化学习过程的基本组成内容
为了便于理解，我们引入任天堂经典游戏——[新超级马里奥兄弟U](https://www.nintendoswitch.com.cn/new_super_mario_bros_u_deluxe/pc/index.html)，作为辅助理解的帮手。作为一个2D横向的闯关游戏，它的状态空间和动作空间无疑是简单的。

![](static/4.6.9.3.2.png)

1.智能体(Agent):它与环境交互，可以观察到环境并且做出决策，然后反馈给环境。在马里奥游戏中，能操控的这个马里奥本体就是智能体。

2.环境(Environment):智能体存在并且与其交互的世界。新超级马里奥兄弟U本身，就是一个环境。

3.状态(State)：对环境当前所处环境的全部描述，记为 $S$。在马里奥游戏中，上面的这张图片就是在本时刻的状态。

4.动作(Action):智能体可以采取的行为，记为 $a$。在马里奥游戏中，马里奥能采取的动作只有：上、左、右三个。这属于**离散动作**，动作数量是有限的。而在机器人控制中，机器人能采取的动作是无限的，这属于**连续动作**。

5.策略(Policy):智能体采取动作的规则，分为**确定性策略**与**随机性策略**。确定性策略代表在相同的状态下，智能体所输出的动作是唯一的。而随机性策略哪怕是在相同的状态下，输出的动作也有可能不一样。这么说有点过于抽象了，那么请思考这个问题：在下面这张图的环境中，如果执行确定性策略会发生什么？(提示：着重关注两个灰色的格子)

![](static/4.6.9.3.3.png)

因此，在强化学习中我们一般使用随机性策略。随机性策略通过引入一定的随机性，使环境能够被更好地探索。同时，如果策略固定——你的对手很容易能预测你的下一步动作并予以反击，这在博弈中是致命的。
随机性策略$\pi$定义如下：

<center>

$\pi(\mathrm{a} \mid \mathrm{s})=P(A=a \mid S=s)$

</center>

这代表着在给定状态s下，作出动作$a$的概率密度。举个例子，在马里奥游戏中，定义动作 $a_{1}$="left",$a_{2}$="right",$a_{3}$="down",动作空间 $a$={$a_{1}$,$a_{2}$,$a_{3}$}。<br>
其中，假设$\pi(\mathrm{a_{1}} \mid \mathrm{s})=0.7$，$\pi(\mathrm{a_{2}} \mid \mathrm{s})=0.2$，$\pi(\mathrm{a_{3}} \mid \mathrm{s})=0.1$。这就代表着，在给定状态s下，执行动作$a_{1}$的概率为0.7，执行动作$a_{2}$的概率为0.2，执行动作$a_{3}$的概率为0.1，智能体随机抽样，依据概率执行动作。也就是说，马里奥左、右、上三个动作都有可能被执行，无非是执行几率大不大的问题。很显然，在知道策略$\pi$的情况下，就可以指导智能体“打游戏”了，学习策略$\pi$是强化学习的最终目标之一，这种方法被称为**基于策略的强化学习**。

6.奖励(Reward):这是一种反馈信号，用于表现智能体与环境交互后"表现"如何。在不同的环境中，我们需要设置不同的奖励。比如，在围棋游戏中，最后赢得游戏才会获得一个奖励。比如在量化交易中，可以直接拿收益亏损作为奖励。拿我们的马里奥游戏举例，吃到金币可以获得较小的奖励，最终通关游戏会获得一个极大的奖励，这样使得智能体以通关为目标、以吃金币为锦上添花。当然了，如果碰到怪物或者是死亡，需要设置一个极大的负奖励，因为这将直接导致游戏结束。

我们可以得出一个结论:每一个奖励 $R_{i}$，都与当时刻的状态 $S_{i}$ 与动作 $A_{i}$ 有关。拿马里奥游戏举例，在当前状态下，是否采取什么样的动作就会决定获得什么样的奖励？马里奥如果采取"向上"，就可以获得金币奖励。如果采取"向右"，碰到小怪会死掉，会获得一个很大的负奖励。如果采取"向左"，那么可能什么事情都不会发生。

7.状态转移(State transition):环境可不会在原地等你。在你操控马里奥执行一个动作后，比如"left"，那屏幕上显示的画面肯定会改变，这就发生了一个状态转移。状态转移函数记作

<center>

$p\left(s^{\prime} \mid s, a\right)=P\left(S^{\prime}=s^{\prime} \mid S=s, A=a\right)$

</center>

状态转移可以是固定的，也可以是随机的，我们通常讨论的是随机的情况。从公式的形式上也可以看出来，这还是一个概率密度函数。这代表着在观测到当前的状态$s$以及动作$a$后，状态转移函数输出新状态$s'$的概率，这个转移函数是只有环境、也就是游戏本身才知道的。比如在超级马里奥兄弟中，操控马里奥执行动作"left"后，敌人"板栗仔"可能向左也可能向右，比如说向左概率为0.8，向右概率为0.2，但是要注意这个概率只有游戏程序本身才知道。敌人动作的不确定性也就导致了环境的不确定性。

知道了上述几个概念，构建强化学习的基本过程就尽在掌握之中了。我们可以构建一个(state,action,reward)轨迹，即：<br>
i.观察到状态$s_{1}$<br>
ii.执行动作$a_{1}$，发生状态转移<br>
iii.观察新状态$s_{2}$与得到奖励$r_{1}$<br>
iv.执行动作$a_{2}$，发生状态转移<br>
v.不断迭代......

该序列轨迹写作：$\langle s_{1},a_{1},r_{1},s_{2},a_{2},r_{2},\ldots,s_{T},a_{T},r_{T} \rangle$

## 强化学习的基本概念
在阅读了前两个小节后，你可能对于强化学习的基本过程以及基本组成内容有了初步的了解。下面将进行强化学习基本概念的介绍，本章节与"基本组成内容"小节是继承关系，请一起阅读。(注:标题真难取，其实上一章就是强化学习的基本元素，这一章为基础元素推导出的基础概念)

1.回报(Retrun)，需要与奖励区分开来。回报又称为"未来的累计奖励"(Cumulative future reward)，这可以在其定义中窥见端倪:

<center>

$U_{\mathrm{t}}=R_{t}+R_{t+1}+R_{t+2}+R_{t+3}+\ldots . R_{t+n}$

</center>

但是这个定义有一个很明显的问题，未来时刻的奖励和现在的一样重要吗？如果我承诺未来给你100块钱，这份**承诺**在你心里的分量和现在就给你100块钱能够等价吗？很明显不能。因此我们引入折扣因子 $\gamma$ ,用以对未来的奖励做出一个折扣。定义折扣回报(Cumulative Discounted future reward)如下：

<center>

$U_{t}=R_{t}+\gamma R_{t+1}+\gamma^{2} R_{t+2}+\ldots \gamma^{n} R_{t+n}$

</center>

这是我们在强化学习中经常使用的概念。其中，折扣率是一个超参数，会对强化学习的结果造成一定的影响。

**注意格式**:如果游戏结束，每一个时刻的奖励都被观测到了——即站在任意时刻，一直到游戏结束的奖励都是可被观测的状态，那么奖励使用小写字母 $r$ 表示。如果游戏还没有结束，未来的奖励还是一个随机变量，那么我们使用大写字母 $R$ 来表示奖励。由于回报是由奖励组成的，那么我们也理所当然地用大写字母 $U_{t}$ 来表示回报。

*Fix:真的理所当然吗？*<br>
让我们回顾一下，之前讲述"奖励"的定义时，我们得出过一个结论:每一个奖励 $R_{i}$，都与当时刻的状态 $S_{i}$ 与动作 $A_{i}$ 有关。我们又知道，状态 $S_{i}$ 与动作 $A_{i}$ 在某种意义上都是随机变量，不要忘了:<br>
1.状态$S_{i}$是由状态转移函数，随机抽样得到的<br>
2.动作$A_{i}$是由策略 $\pi$ ,以状态$S_{i}$作为输入后随机抽样输出的

因此，$U_{t}$ 就跟 $t$ 时刻开始未来所有的状态和动作都有关，$U_{t}$的随机性也因此和未来所有的状态和动作有关。

2.动作价值函数(Action-Value Function)

$U_{t}$ 在强化学习过程中的重要性不言而喻，这就代表着总体奖励——可以用于衡量智能体总的表现水平，并且智能体的目标就是让这个回报越大越好。但是由于我们前面说过的原因，回报 $U_{t}$ 受制于状态与动作，是一个随机变量。也就是说，在 $t$ 时刻，我们无法得知 $U_{t}$ 究竟是什么。有没有一种办法，能够消除掉随机性？很自然的，我们想起了《概率论与数理统计》中的期望。从数学上来说，对 $U_{t}$ 在策略函数 $\pi$ 下求期望，就可以消掉里边所有的随机性。因此，我们得到动作价值函数 $Q_\pi$ 的定义如下：

<center>

$Q_\pi=E\left(U_t \mid S_t=s_t, A_t=a_t\right)$

</center>

动作价值函数 $Q_\pi$ 消除了不确定的未来的动作和状态，转而把已观测到的状态 $s_{t}$ 和动作 $a_{t} $ 作为被观测的变量而非随机变量来对待。动作价值函数带来的意义就在于，能够在策略 $\pi$ 下，对于当前状态 $s$ 下所有动作 $a$ 进行打分，基于分数我们就可以知道哪个动作好、哪个动作不好。

3.最优动作价值函数(Optimal action-value function)

动作价值函数对于回报 $U_{t}$ 关于策略 $\pi$ 求取了期望，成功地消去了状态以及动作的随机性。但是需要注意的是，使用不同的策略 $\pi$ 就会得到不同的动作价值函数 $Q_\pi$ ——其实质上受到三个参数影响,即($\pi$，$s$，$a$)。我们应该使用"效果最好"的那种函数，也就是能让 $Q_\pi$ 最大化的那个 $\pi$ ，基于此我们可以得到最优动作价值函数：

<center>

$Q^*\left(s_t, a_t\right)= \underset{\pi}{max} Q_\pi\left(s_t, a_t\right)$

</center>

我们跨出了历史性的一步。

如果有了 $Q^*$ 函数，意味着可以评价动作的好坏了。我们的价值函数不再和策略有关，在观测的状态 $s$ 下，$Q^*$函数成为指挥智能体动作的“指挥官”——哪个动作的分数最高，智能体就应该执行哪个动作。学习 $Q^*$ 函数也是强化学习的最终目标之一，我们可以维护一张 $Q$ 表
